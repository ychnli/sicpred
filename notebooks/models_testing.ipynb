{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "import pickle\n",
    "#import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import pandas as pd\n",
    "#import h5py\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If available, print the name of the GPU\n",
    "if cuda_available:\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "DATA_DIRECTORY = '/oak/stanford/groups/earlew/yuchen'\n",
    "\n",
    "def save_dict_to_pickle(data_dict, save_path):\n",
    "    \"\"\"\n",
    "    Saves a dictionary to a specified path using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dict (dict): The dictionary to save.\n",
    "    save_path (str): The file path where the dictionary will be saved.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "\n",
    "class SeaIceDataset(Dataset):\n",
    "    def __init__(self, data_directory, configuration, split_array, start_prediction_months, \\\n",
    "                split_type='train', target_shape=(336, 320)):\n",
    "        self.data_directory = data_directory\n",
    "        self.configuration = configuration\n",
    "        self.split_array = split_array\n",
    "        self.start_prediction_months = start_prediction_months\n",
    "        self.split_type = split_type\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "        # Open the HDF5 files\n",
    "        self.inputs_file = h5py.File(f\"{data_directory}/inputs_{configuration}.h5\", 'r')\n",
    "        self.targets_file = h5py.File(f\"{data_directory}/targets.h5\", 'r')\n",
    "        \n",
    "        self.inputs = self.inputs_file[f\"inputs_{configuration}\"]\n",
    "        self.targets = self.targets_file['targets_sea_ice_only']\n",
    "\n",
    "        self.n_samples, self.n_channels, self.n_y, self.n_x = self.inputs.shape\n",
    "        \n",
    "        # Get indices for the specified split type\n",
    "        self.indices = np.where(self.split_array == split_type)[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        input_data = self.inputs[actual_idx]\n",
    "        target_data = self.targets[actual_idx]\n",
    "        start_prediction_month = self.start_prediction_months[actual_idx]\n",
    "\n",
    "        # Pad input_data and target_data to the target shape\n",
    "        pad_y = self.target_shape[0] - self.n_y\n",
    "        pad_x = self.target_shape[1] - self.n_x\n",
    "        input_data = np.pad(input_data, ((0, 0), (pad_y//2, pad_y//2), (pad_x//2, pad_x//2)), mode='constant', constant_values=0)\n",
    "        target_data = np.pad(target_data, ((0, 0), (pad_y//2, pad_y//2), (pad_x//2, pad_x//2)), mode='constant', constant_values=0)\n",
    "\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target_data, dtype=torch.float32)\n",
    "\n",
    "        # Get the target months for this sample\n",
    "        target_months = pd.date_range(start=start_prediction_month, end=start_prediction_month + pd.DateOffset(months=5), freq=\"MS\")\n",
    "        target_months = target_months.month.to_numpy()\n",
    "        \n",
    "        return input_tensor, target_tensor, target_months\n",
    "\n",
    "    def __del__(self):\n",
    "        self.inputs_file.close()\n",
    "        self.targets_file.close()\n",
    "\n",
    "class UNetRes3(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a UNet of resolution 3 (nomenclature from Williams et al. 2023)\n",
    "    The resolution is defined as the number of encoder/decoder blocks. The\n",
    "    number at the end of encoder and decoder blocks denote their depth in \n",
    "    the network (thus we have, E1 -> E2 -> E3 -> D3 -> D2 -> D1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, device, spatial_shape=(336, 320), n_channels_factor=1, filter_size=3):\n",
    "        super(UNetRes3, self).__init__()\n",
    "        \n",
    "        self.encoder1 = self.conv_block(in_channels, int(64 * n_channels_factor), filter_size)\n",
    "        self.encoder2 = self.conv_block(int(64 * n_channels_factor), int(128 * n_channels_factor), filter_size)\n",
    "        self.encoder3 = self.conv_block(int(128 * n_channels_factor), int(256 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.bottleneck = self.conv_block(int(256 * n_channels_factor), int(512 * n_channels_factor), filter_size)\n",
    "        \n",
    "        self.decoder3_conv = self.conv(int(512 * n_channels_factor), int(256 * n_channels_factor), filter_size)\n",
    "        self.decoder3_conv_block = self.conv_block(2 * int(256 * n_channels_factor), int(256 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.decoder2_conv = self.conv(int(256 * n_channels_factor), int(128 * n_channels_factor), filter_size)\n",
    "        self.decoder2_conv_block = self.conv_block(2 * int(128 * n_channels_factor), int(128 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.decoder1_conv_1 = self.conv(int(128 * n_channels_factor), int(64 * n_channels_factor), filter_size)\n",
    "        self.decoder1_conv_2 = self.conv(int(64 * n_channels_factor), int(64 * n_channels_factor), filter_size)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(int(64 * n_channels_factor), out_channels, kernel_size=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # Make a land mask tensor that is the same shape as the output tensor\n",
    "        self.land_mask = self.create_land_mask(spatial_shape).to(device)\n",
    "    \n",
    "    def create_land_mask(self, spatial_shape):\n",
    "        land_mask_npy = ~xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/land_mask.nc\").mask.data\n",
    "        n_y, n_x = land_mask_npy.shape\n",
    "        pad_y = spatial_shape[0] - n_y\n",
    "        pad_x = spatial_shape[1] - n_x\n",
    "        land_mask_npy = np.pad(land_mask_npy, ((pad_y//2, pad_y//2), (pad_x//2, pad_x//2)), mode='constant', constant_values=0)\n",
    "        \n",
    "        return torch.from_numpy(land_mask_npy).unsqueeze(0).repeat(6, 1, 1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, filter_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def conv(self, in_channels, out_channels, filter_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        bottleneck = self.bottleneck(self.pool(enc3))\n",
    "        \n",
    "        dec3 = self.upsample(bottleneck)\n",
    "        dec3 = self.decoder3_conv(dec3)\n",
    "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
    "        dec3 = self.decoder3_conv_block(dec3)\n",
    "        \n",
    "        dec2 = self.upsample(dec3)\n",
    "        dec2 = self.decoder2_conv(dec2)\n",
    "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
    "        dec2 = self.decoder2_conv_block(dec2)\n",
    "        \n",
    "        dec1 = self.upsample(dec2)\n",
    "        dec1 = self.decoder1_conv_1(dec1)\n",
    "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
    "        dec1 = self.decoder1_conv_1(dec1)\n",
    "        dec1 = self.decoder1_conv_2(dec1)\n",
    "        dec1 = self.decoder1_conv_2(dec1)\n",
    "\n",
    "        output = torch.sigmoid(self.final_conv(dec1))\n",
    "        \n",
    "        # Apply the land mask\n",
    "        output = output * self.land_mask\n",
    "        \n",
    "        return output\n",
    "\n",
    "class UNetRes4(UNetRes3):\n",
    "    def __init__(self, in_channels, out_channels, device, spatial_shape=(336, 320), n_channels_factor=1, filter_size=3):\n",
    "        super(UNetRes4, self).__init__(in_channels, out_channels, device, spatial_shape, n_channels_factor, filter_size)\n",
    "\n",
    "        self.encoder4 = self.conv_block(int(256 * n_channels_factor), int(512 * n_channels_factor), filter_size)\n",
    "        self.bottleneck = self.conv_block(int(512 * n_channels_factor), int(1024 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.decoder4_conv = self.conv(int(1024 * n_channels_factor), int(512 * n_channels_factor), filter_size)\n",
    "        self.decoder4_conv_block = self.conv_block(2 * int(512 * n_channels_factor), int(512 * n_channels_factor), filter_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        enc4 = self.encoder4(self.pool(enc3))\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.upsample(bottleneck)\n",
    "        dec4 = self.decoder4_conv(dec4)\n",
    "        dec4 = torch.cat((enc4, dec4), dim=1)\n",
    "        dec4 = self.decoder4_conv_block(dec4)\n",
    "        \n",
    "        dec3 = self.upsample(dec4)\n",
    "        dec3 = self.decoder3_conv(dec3)\n",
    "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
    "        dec3 = self.decoder3_conv_block(dec3)\n",
    "        \n",
    "        dec2 = self.upsample(dec3)\n",
    "        dec2 = self.decoder2_conv(dec2)\n",
    "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
    "        dec2 = self.decoder2_conv_block(dec2)\n",
    "        \n",
    "        dec1 = self.upsample(dec2)\n",
    "        dec1 = self.decoder1_conv_1(dec1)\n",
    "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
    "        dec1 = self.decoder1_conv_1(dec1)\n",
    "        dec1 = self.decoder1_conv_2(dec1)\n",
    "        dec1 = self.decoder1_conv_2(dec1)\n",
    "\n",
    "        output = torch.sigmoid(self.final_conv(dec1))\n",
    "        \n",
    "        # Apply the land mask\n",
    "        output = output * self.land_mask\n",
    "        \n",
    "        return output\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self, use_weights, zero_class_weight=None):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.ice_mask = xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/monthly_ice_mask.nc\").mask\n",
    "        self.use_weights = use_weights\n",
    "        self.zero_class_weight = zero_class_weight\n",
    "\n",
    "    def forward(self, outputs, targets, target_months):\n",
    "        n_active_cells = 0\n",
    "\n",
    "        for target_months_subset in target_months:\n",
    "            n_active_cells += self.ice_mask.sel(month=target_months_subset.cpu()).sum().values\n",
    "        \n",
    "        # Punish predictions of sea ice in ice free zones \n",
    "        if self.use_weights:\n",
    "            weights = torch.where(targets == 0, self.zero_class_weight, 1)\n",
    "            loss = torch.sum(((targets - outputs) ** 2) * weights) / n_active_cells\n",
    "        else:\n",
    "            loss = torch.sum((targets - outputs) ** 2) / n_active_cells\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "TRAIN_MONTHS = pd.date_range(start='1981-01-01', end='2014-12-01', freq='MS')\n",
    "VAL_MONTHS = pd.date_range(start='2015-01-01', end='2018-12-01', freq='MS')\n",
    "TEST_MONTHS = pd.date_range(start='2019-01-01', end='2024-06-01', freq='MS')\n",
    "\n",
    "def print_split_stats(split_array):\n",
    "    ntrain = sum(split_array == 'train')\n",
    "    nval = sum(split_array == 'val')\n",
    "    ntest = sum(split_array == 'test')\n",
    "    \n",
    "    print(f\"train samples: {ntrain} ({round(ntrain / len(split_array), 2)})\")\n",
    "    print(f\"val samples: {nval} ({round(nval / len(split_array), 2)})\")\n",
    "    print(f\"test samples: {ntest} ({round(ntest / len(split_array), 2)})\")\n",
    "\n",
    "def generate_start_prediction_months(max_month_lead_time=6, max_input_lag_time=12):\n",
    "    # Construct the date range for the data pairs \n",
    "    # Note that this is not continuous due to the missing data in 1987-1988 \n",
    "    first_range = pd.date_range('1981-01', pd.Timestamp('1987-12') - pd.DateOffset(months=max_month_lead_time+1), freq='MS')\n",
    "    second_range = pd.date_range(pd.Timestamp('1988-01') + pd.DateOffset(months=max_input_lag_time+1), '2024-01', freq='MS')\n",
    "\n",
    "    return first_range.append(second_range)\n",
    "\n",
    "\n",
    "def generate_split_array(verbose=1):\n",
    "    start_prediction_months = generate_start_prediction_months()\n",
    "    split_array = np.empty(np.shape(start_prediction_months), dtype=object)\n",
    "    \n",
    "    for i,month in enumerate(start_prediction_months):\n",
    "        if month in TRAIN_MONTHS: split_array[i] = \"train\"\n",
    "        if month in VAL_MONTHS: split_array[i] = \"val\"\n",
    "        if month in TEST_MONTHS: split_array[i] = \"test\"\n",
    "\n",
    "    if verbose == 2: print_split_stats(split_array)\n",
    "    \n",
    "    return split_array, start_prediction_months\n",
    "    \n",
    "\n",
    "\n",
    "model_hyperparam_configs = {\n",
    "    \"name\": \"UNetRes3_simpleinputs_b1lr1\",\n",
    "    \"architecture\": \"UNetRes3\",\n",
    "    \"input_config\": \"simple\", \n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"use_zeros_weight\": False,\n",
    "    \"zeros_weight\": None,\n",
    "    \"early_stopping\": True,\n",
    "    \"patience\": 10,\n",
    "    \"max_epochs\": 100,\n",
    "    \"notes\": \"Testing the model training function and early stopping. \\\n",
    "        Similar to UNetRes3_no_zeros_weight but different batch_size.\"\n",
    "}\n",
    "\n",
    "in_channels_config = {\n",
    "    \"sea_ice_only\": 14,\n",
    "    \"simple\": 23,\n",
    "    \"all\": 40\n",
    "}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNetRes3(in_channels=in_channels_config[model_hyperparam_configs[\"input_config\"]], \\\n",
    "                out_channels=6, device=device, n_channels_factor=1, filter_size=3).to(device)\n",
    "\n",
    "criterion = MaskedMSELoss(use_weights=model_hyperparam_configs[\"use_zeros_weight\"], \\\n",
    "                          zero_class_weight=model_hyperparam_configs[\"zeros_weight\"])\n",
    "\n",
    "if model_hyperparam_configs[\"optimizer\"] == \"adam\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_hyperparam_configs[\"lr\"])\n",
    "else: \n",
    "    raise ValueError(\"Haven't yet configured training procedure for other optimizers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "split_array, start_prediction_months = generate_split_array()\n",
    "data_directory = os.path.join(DATA_DIRECTORY, 'sicpred/data_pairs_npy')\n",
    "configuration = 'simple'\n",
    "batch_size = 4\n",
    "\n",
    "# Create dataset instances for training, validation, and testing\n",
    "train_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, split_type='train', target_shape=(336, 320))\n",
    "val_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, split_type='val', target_shape=(336, 320))\n",
    "test_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, split_type='test', target_shape=(336, 320))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, model_hyperparam_configs, \n",
    "                plot_training_curve=True, verbose=1, save_val_predictions=True):\n",
    "\n",
    "    # read in some metadata\n",
    "    configuration = model_hyperparam_configs[\"input_config\"]\n",
    "    batch_size = model_hyperparam_configs[\"batch_size\"]\n",
    "    early_stopping = model_hyperparam_configs[\"early_stopping\"]\n",
    "    patience = model_hyperparam_configs[\"patience\"]\n",
    "    model_name = model_hyperparam_configs[\"name\"]\n",
    "    num_epochs = model_hyperparam_configs[\"max_epochs\"]\n",
    "\n",
    "    # generate array labeling which months are train/val/test\n",
    "    split_array, start_prediction_months = generate_split_array()\n",
    "\n",
    "    # create dataset instances for training, validation, and testing\n",
    "    data_pairs_directory = os.path.join(DATA_DIRECTORY, 'sicpred/data_pairs_npy')\n",
    "    train_dataset = SeaIceDataset(data_pairs_directory, configuration, split_array, start_prediction_months, split_type='train', target_shape=(336, 320))\n",
    "    val_dataset = SeaIceDataset(data_pairs_directory, configuration, split_array, start_prediction_months, split_type='val', target_shape=(336, 320))\n",
    "    test_dataset = SeaIceDataset(data_pairs_directory, configuration, split_array, start_prediction_months, split_type='test', target_shape=(336, 320))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_predictions = []\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_at_early_stopping = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets, target_months in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, target_months)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        if verbose >= 1: print(f\"Epoch [{epoch+1}/{num_epochs}], train loss: {epoch_loss:.4f}\", end=', ')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, target_months in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets, target_months)\n",
    "                val_loss += loss.item() * inputs.size(0)        \n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        if verbose >= 1: print(f\"validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save the best model weights\n",
    "                os.makedirs(f\"{DATA_DIRECTORY}/sicpred/models\", exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{DATA_DIRECTORY}/sicpred/models/{model_name}.pth\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    if verbose >= 1: print(\"Early stopping triggered\")\n",
    "                    epoch_at_early_stopping = epoch\n",
    "                    break\n",
    "        \n",
    "    # Plot training curves\n",
    "    if plot_training_curve:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(np.arange(0.5, len(train_losses) + 0.5, 1), train_losses, label=\"train\")\n",
    "        plt.plot(np.arange(1, len(val_losses) + 1, 1), val_losses, label=\"val\")\n",
    "        plt.legend()\n",
    "        plt.title(model_name)\n",
    "\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid()\n",
    "\n",
    "        os.makedirs(f\"{DATA_DIRECTORY}/figures/training_curves\", exist_ok=True)\n",
    "        plt.savefig(f\"{DATA_DIRECTORY}/figures/training_curves/{model_name}_training_curve.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # Save the validation predictions for hyperparam tuning \n",
    "    if save_val_predictions:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, target_months in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_predictions.append(outputs.cpu().numpy())\n",
    "        os.makedirs(f\"{DATA_DIRECTORY}/sicpred/val_predictions\", exist_ok=True)\n",
    "        np.save(f\"{DATA_DIRECTORY}/sicpred/val_predictions/{model_name}_val_predictions.npy\", np.concatenate(val_predictions, axis=0))\n",
    "\n",
    "    # Save the trained model\n",
    "    if verbose >= 1: print(\"Saving model weights...\", end='')\n",
    "\n",
    "    # record the best val loss and, if early stopping happened, the last epoch\n",
    "    model_hyperparam_configs[\"best_val_loss\"] = best_val_loss\n",
    "\n",
    "    if epoch_at_early_stopping is not None: \n",
    "        model_hyperparam_configs[\"final_epoch\"] = epoch_at_early_stopping\n",
    "    else:\n",
    "        model_hyperparam_configs[\"final_epoch\"] = num_epochs\n",
    "        \n",
    "    os.makedirs(f\"{DATA_DIRECTORY}/sicpred/val_predictions\", exist_ok=True)\n",
    "    save_dict_to_pickle(model_hyperparam_configs, f\"{DATA_DIRECTORY}/sicpred/models/{model_name}.pkl\")\n",
    "    torch.save(model.state_dict(), f\"{DATA_DIRECTORY}/sicpred/models/{model_name}.pth\")\n",
    "    if verbose >= 1: print(\"done! \\n\\n\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "train_model(model, device, model_hyperparam_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_pickle(load_path):\n",
    "    with open(load_path, 'rb') as file:\n",
    "        data_dict = pickle.load(file)\n",
    "    return data_dict\n",
    "\n",
    "model_name = f\"UNetRes3_simpleinputs_b1lr1\"\n",
    "\n",
    "predictions = np.load(f\"{DATA_DIRECTORY}/sicpred/val_predictions/{model_name}_val_predictions.npy\")\n",
    "\n",
    "with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nsidc_sic = xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/seaice_conc_monthly_all.nc\")\n",
    "\n",
    "model_state_path = os.path.join(f\"{DATA_DIRECTORY}/sicpred/models/{model_full_name}.pth\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_state_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "inputs_list = []\n",
    "outputs_list = []\n",
    "targets_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, target_months in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        inputs_list.append(inputs)\n",
    "        outputs_list.append(outputs)\n",
    "        targets_list.append(targets)\n",
    "        loss = criterion(outputs, targets, target_months)\n",
    "        print(loss.item() * inputs.size(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12.5, 5))\n",
    "\n",
    "month_index = 0\n",
    "lead_time = 0\n",
    "list_elem = 7\n",
    "\n",
    "ax1.contourf(nsidc_sic.xgrid, nsidc_sic.ygrid, outputs_list[list_elem][month_index,lead_time,2:334,2:318].cpu(), cmap='nipy_spectral',\\\n",
    "    levels=np.arange(0,1.025,0.025))\n",
    "cax = ax2.contourf(nsidc_sic.xgrid, nsidc_sic.ygrid, targets_list[list_elem][month_index,lead_time,2:334,2:318].cpu(), cmap='nipy_spectral',\\\n",
    "    levels=np.arange(0,1.025,0.025))\n",
    "\n",
    "ax1.set_title(\"Prediction\")\n",
    "ax2.set_title(\"Truth\")\n",
    "\n",
    "cbar = fig.colorbar(cax, ax=[ax1, ax2], orientation='vertical')\n",
    "cbar.set_label('Sea ice concentration')\n",
    "\n",
    "predicted_month = VAL_MONTHS[month_index + lead_time + list_elem*batch_size]\n",
    "\n",
    "plt.suptitle(rf\"Prediction for {predicted_month.month:02}/{predicted_month.year} at {lead_time+1} lead month\", fontsize=16)\n",
    "#plt.savefig(f\"../figures/sample_predictions/{model_full_name}_{predicted_month.month:02}_{predicted_month.year}_lead_{lead_time+1}.jpg\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
