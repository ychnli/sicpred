{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device name: NVIDIA A100-SXM4-80GB\n",
      "Device count: 1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import xarray as xr\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "from netCDF4 import Dataset\n",
    "import h5py\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If available, print the name of the GPU\n",
    "if cuda_available:\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "DATA_DIRECTORY = '/oak/stanford/groups/earlew/yuchen'\n",
    "\n",
    "def save_dict_to_pickle(data_dict, save_path):\n",
    "    \"\"\"\n",
    "    Saves a dictionary to a specified path using pickle.\n",
    "    \n",
    "    Parameters:\n",
    "    data_dict (dict): The dictionary to save.\n",
    "    save_path (str): The file path where the dictionary will be saved.\n",
    "    \"\"\"\n",
    "    with open(save_path, 'wb') as file:\n",
    "        pickle.dump(data_dict, file)\n",
    "\n",
    "class SeaIceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_directory, configuration, split_array, start_prediction_months, \\\n",
    "                split_type='train', target_shape=(336, 320), mode=\"regression\", class_splits=None):\n",
    "        self.data_directory = data_directory\n",
    "        self.configuration = configuration\n",
    "        self.split_array = split_array\n",
    "        self.start_prediction_months = start_prediction_months\n",
    "        self.split_type = split_type\n",
    "        self.target_shape = target_shape\n",
    "        self.class_splits = class_splits\n",
    "        self.mode = mode\n",
    "\n",
    "        # Open the HDF5 files\n",
    "        self.inputs_file = h5py.File(f\"{data_directory}/inputs_{configuration}.h5\", 'r')\n",
    "        self.targets_file = h5py.File(f\"{data_directory}/targets_regression.h5\", 'r')\n",
    "        \n",
    "        self.inputs = self.inputs_file[f\"inputs_{configuration}\"]\n",
    "        self.targets = self.targets_file['targets_sea_ice_only']\n",
    "\n",
    "        self.n_samples, self.n_channels, self.n_y, self.n_x = self.inputs.shape\n",
    "        \n",
    "        # Get indices for the specified split type\n",
    "        self.indices = np.where(self.split_array == split_type)[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        input_data = self.inputs[actual_idx]\n",
    "        target_data = self.targets[actual_idx]\n",
    "        start_prediction_month = self.start_prediction_months[actual_idx]\n",
    "\n",
    "        # Pad input_data and target_data to the target shape\n",
    "        pad_y = self.target_shape[0] - self.n_y\n",
    "        pad_x = self.target_shape[1] - self.n_x\n",
    "        input_data = np.pad(input_data, ((0, 0), (pad_y//2, pad_y//2), (pad_x//2, pad_x//2)), mode='constant', constant_values=0)\n",
    "        target_data = np.pad(target_data, ((0, 0), (pad_y//2, pad_y//2), (pad_x//2, pad_x//2)), mode='constant', constant_values=0)\n",
    "\n",
    "        # If we are doing classification, then discretise the target data\n",
    "        if self.mode == \"classification\":\n",
    "            if self.class_splits is None:\n",
    "                raise ValueError(\"need to specify a monotonically increasing list class_splits denoting class boundaries\")\n",
    "\n",
    "            # check if class_split is monotonically increasing\n",
    "            if len(self.class_splits) > 1 and np.any(np.diff(self.class_splits) < 0): \n",
    "                raise ValueError(\"class_splits needs to be monotonically increasing\")\n",
    "\n",
    "            bounds = [] # bounds for classes\n",
    "            for i,class_split in enumerate(self.class_splits): \n",
    "                if i == 0: \n",
    "                    bounds.append([0, class_split])\n",
    "                if i == len(self.class_splits) - 1: \n",
    "                    bounds.append([class_split, 1])\n",
    "                else: \n",
    "                    bounds.append([class_split, self.class_splits[i+1]])\n",
    "            \n",
    "            target_classes_data = np.zeros_like(target_data) \n",
    "            target_classes_data = target_classes_data[np.newaxis,:,:,:]\n",
    "            target_classes_data = np.repeat(target_classes_data, len(bounds), axis=0)\n",
    "            for i,bound in enumerate(bounds): \n",
    "                if i == len(bounds) - 1: \n",
    "                    target_classes_data[i,:,:,:] = np.logical_and(target_data >= bound[0], target_data <= bound[1]).astype(int)\n",
    "                else:\n",
    "                    target_classes_data[i,:,:,:] = np.logical_and(target_data >= bound[0], target_data < bound[1]).astype(int)\n",
    "            \n",
    "            target_data = target_classes_data \n",
    "\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target_data, dtype=torch.float32)\n",
    "\n",
    "        # Get the target months for this sample\n",
    "        target_months = pd.date_range(start=start_prediction_month, end=start_prediction_month + pd.DateOffset(months=5), freq=\"MS\")\n",
    "        target_months = target_months.month.to_numpy()\n",
    "        \n",
    "        return input_tensor, target_tensor, target_months\n",
    "\n",
    "    def __del__(self):\n",
    "        self.inputs_file.close()\n",
    "        self.targets_file.close()\n",
    "\n",
    "class UNetRes3(nn.Module):\n",
    "    \"\"\"\n",
    "    Builds a UNet of resolution 3 (nomenclature from Williams et al. 2023)\n",
    "    The resolution is defined as the number of encoder/decoder blocks. The\n",
    "    number at the end of encoder and decoder blocks denote their depth in \n",
    "    the network (thus we have, E1 -> E2 -> E3 -> B -> D3 -> D2 -> D1) where\n",
    "    B is the bottleneck block\n",
    "\n",
    "    mode: (str) 'regression' or 'classification'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mode, device, spatial_shape=(336, 320), \n",
    "                n_channels_factor=1, filter_size=3, T=1.0, n_classes=2):\n",
    "\n",
    "        super(UNetRes3, self).__init__()\n",
    "        # Model parameters\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.mode = mode \n",
    "        self.n_classes = n_classes\n",
    "        self.spatial_shape = spatial_shape\n",
    "        self.T = T # temperature scaling factor \n",
    "\n",
    "        # Model layers\n",
    "        self.encoder1 = self.conv_block(in_channels, int(64 * n_channels_factor), filter_size)\n",
    "        self.encoder2 = self.conv_block(int(64 * n_channels_factor), int(128 * n_channels_factor), filter_size)\n",
    "        self.encoder3 = self.conv_block(int(128 * n_channels_factor), int(256 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.bottleneck = self.conv_block(int(256 * n_channels_factor), int(512 * n_channels_factor), filter_size)\n",
    "        \n",
    "        self.decoder3_conv = self.conv(int(512 * n_channels_factor), int(256 * n_channels_factor), filter_size)\n",
    "        self.decoder3_conv_block = self.conv_block(2 * int(256 * n_channels_factor), int(256 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.decoder2_conv = self.conv(int(256 * n_channels_factor), int(128 * n_channels_factor), filter_size)\n",
    "        self.decoder2_conv_block = self.conv_block(2 * int(128 * n_channels_factor), int(128 * n_channels_factor), filter_size)\n",
    "\n",
    "        self.decoder1_conv_1 = self.conv(int(128 * n_channels_factor), int(64 * n_channels_factor), filter_size)\n",
    "        self.decoder1_conv_2 = self.conv(int(64 * n_channels_factor), int(64 * n_channels_factor), filter_size)\n",
    "        \n",
    "        self.final_conv_reg = nn.Conv2d(int(64 * n_channels_factor), out_channels, kernel_size=1)\n",
    "        self.final_convs_class = nn.ModuleList([nn.Conv2d(int(64 * n_channels_factor), n_classes, kernel_size=1)\n",
    "                                                for i in range(out_channels)])\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "        # Make a land mask tensor that is the same shape as the output tensor\n",
    "        self.land_mask = self.create_land_mask().to(device)\n",
    "    \n",
    "    def create_land_mask(self):\n",
    "        land_mask_npy = ~xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/land_mask.nc\").mask.data\n",
    "        n_y, n_x = land_mask_npy.shape\n",
    "        pad_y = self.spatial_shape[0] - n_y\n",
    "        pad_x = self.spatial_shape[1] - n_x\n",
    "        land_mask_npy = np.pad(land_mask_npy, ((pad_y//2, pad_y//2), (pad_x//2, pad_x//2)), mode='constant', constant_values=0)\n",
    "        \n",
    "        return torch.from_numpy(land_mask_npy).unsqueeze(0).repeat(6, 1, 1)\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, filter_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def set_temperature(self, T):\n",
    "        self.T = T\n",
    "    \n",
    "    def conv(self, in_channels, out_channels, filter_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        bottleneck = self.bottleneck(self.pool(enc3))\n",
    "        \n",
    "        dec3 = self.upsample(bottleneck)\n",
    "        dec3 = self.decoder3_conv(dec3)\n",
    "        dec3 = torch.cat((enc3, dec3), dim=1)\n",
    "        dec3 = self.decoder3_conv_block(dec3)\n",
    "        \n",
    "        dec2 = self.upsample(dec3)\n",
    "        dec2 = self.decoder2_conv(dec2)\n",
    "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
    "        dec2 = self.decoder2_conv_block(dec2)\n",
    "        \n",
    "        dec1 = self.upsample(dec2)\n",
    "        dec1 = self.decoder1_conv_1(dec1)\n",
    "        dec1 = torch.cat((enc1, dec1), dim=1)\n",
    "        dec1 = self.decoder1_conv_1(dec1)\n",
    "        dec1 = self.decoder1_conv_2(dec1)\n",
    "        dec1 = self.decoder1_conv_2(dec1)\n",
    "\n",
    "        if self.mode == \"regression\":\n",
    "            output = torch.sigmoid(self.final_conv_reg(dec1))\n",
    "            \n",
    "            # Apply the land mask\n",
    "            output = output * self.land_mask\n",
    "\n",
    "        elif self.mode == \"classification\": \n",
    "            final_logits = torch.stack([self.final_convs_class[i](dec1) for i in range(self.out_channels)], dim=2)\n",
    "            final_logits = final_logits.view(-1, self.n_classes, 6, self.spatial_shape[0], self.spatial_shape[1])\n",
    "            final_logits = final_logits / self.T  # Apply temperature scaling\n",
    "\n",
    "            output = F.softmax(final_logits, dim=2)  \n",
    "\n",
    "            # This adds batch size and class dimensions \n",
    "            land_mask = self.land_mask.unsqueeze(0).unsqueeze(0) \n",
    "\n",
    "            # For the no sea ice class, the land should be automatically assigned probability 1  \n",
    "            # To do this, first clear all land values by multiplying the mask and then add 1\n",
    "            output = output * land_mask\n",
    "            output[:,0,:,:,:] += ~land_mask[:,0,:,:,:]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoricalFocalLoss(nn.Module):\n",
    "    def __init__(self, device, gamma=2., weight_by_month=True): \n",
    "        super(CategoricalFocalLoss, self).__init__()\n",
    "        self.ice_mask = xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/monthly_ice_mask.nc\").mask\n",
    "        self.epsilon = 1e-10\n",
    "        self.gamma = gamma \n",
    "        self.device = device\n",
    "        self.weight_by_month = weight_by_month\n",
    "\n",
    "        if weight_by_month: self.monthly_weights = self.calculate_monthly_weight()\n",
    "\n",
    "    def calculate_monthly_weight(self):\n",
    "        monthly_max_ice_cells = self.ice_mask.sum(dim=(\"x\",\"y\"))\n",
    "\n",
    "        return monthly_max_ice_cells / np.min(monthly_max_ice_cells)\n",
    "    \n",
    "    def forward(self, outputs, targets, target_months):\n",
    "        # clip 0 values to prevent inf during log step \n",
    "        outputs = torch.where(outputs == 0, outputs + self.epsilon, outputs) \n",
    "        \n",
    "        cross_entropy = -targets * torch.log(outputs)\n",
    "\n",
    "        # N x n_classes x 6 x d1 x d2 \n",
    "        focal_loss = ((targets) ** self.gamma) * cross_entropy \n",
    "\n",
    "        if self.weight_by_month:\n",
    "            for i,batch_target_months in enumerate(target_months):\n",
    "                # Make a tensor of the monthly weights\n",
    "                weights = torch.tensor(self.monthly_weights.sel(month=batch_target_months.cpu()).values).to(self.device)\n",
    "                weights = weights.unsqueeze(0).unsqueeze(2).unsqueeze(2)\n",
    "\n",
    "                focal_loss[i,:,:,:,:] *= weights\n",
    "\n",
    "        return torch.mean(focal_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self, use_weights, zero_class_weight=None):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.ice_mask = xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/monthly_ice_mask.nc\").mask\n",
    "        self.use_weights = use_weights\n",
    "        self.zero_class_weight = zero_class_weight\n",
    "\n",
    "    def forward(self, outputs, targets, target_months):\n",
    "        n_active_cells = 0\n",
    "\n",
    "        for target_months_subset in target_months:\n",
    "            n_active_cells += self.ice_mask.sel(month=target_months_subset.cpu()).sum().values\n",
    "        \n",
    "        # Punish predictions of sea ice in ice free zones \n",
    "        if self.use_weights:\n",
    "            weights = torch.where(targets == 0, self.zero_class_weight, 1)\n",
    "            loss = torch.sum(((targets - outputs) ** 2) * weights) / n_active_cells\n",
    "        else:\n",
    "            loss = torch.sum((targets - outputs) ** 2) / n_active_cells\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "TRAIN_MONTHS = pd.date_range(start='1981-01-01', end='2014-12-01', freq='MS')\n",
    "VAL_MONTHS = pd.date_range(start='2015-01-01', end='2018-12-01', freq='MS')\n",
    "TEST_MONTHS = pd.date_range(start='2019-01-01', end='2024-06-01', freq='MS')\n",
    "\n",
    "def print_split_stats(split_array):\n",
    "    ntrain = sum(split_array == 'train')\n",
    "    nval = sum(split_array == 'val')\n",
    "    ntest = sum(split_array == 'test')\n",
    "    \n",
    "    print(f\"train samples: {ntrain} ({round(ntrain / len(split_array), 2)})\")\n",
    "    print(f\"val samples: {nval} ({round(nval / len(split_array), 2)})\")\n",
    "    print(f\"test samples: {ntest} ({round(ntest / len(split_array), 2)})\")\n",
    "\n",
    "def generate_start_prediction_months(max_month_lead_time=6, max_input_lag_time=12):\n",
    "    # Construct the date range for the data pairs \n",
    "    # Note that this is not continuous due to the missing data in 1987-1988 \n",
    "    first_range = pd.date_range('1981-01', pd.Timestamp('1987-12') - pd.DateOffset(months=max_month_lead_time+1), freq='MS')\n",
    "    second_range = pd.date_range(pd.Timestamp('1988-01') + pd.DateOffset(months=max_input_lag_time+1), '2024-01', freq='MS')\n",
    "\n",
    "    return first_range.append(second_range)\n",
    "\n",
    "\n",
    "def generate_split_array(verbose=1):\n",
    "    start_prediction_months = generate_start_prediction_months()\n",
    "    split_array = np.empty(np.shape(start_prediction_months), dtype=object)\n",
    "    \n",
    "    for i,month in enumerate(start_prediction_months):\n",
    "        if month in TRAIN_MONTHS: split_array[i] = \"train\"\n",
    "        if month in VAL_MONTHS: split_array[i] = \"val\"\n",
    "        if month in TEST_MONTHS: split_array[i] = \"test\"\n",
    "\n",
    "    if verbose == 2: print_split_stats(split_array)\n",
    "    \n",
    "    return split_array, start_prediction_months\n",
    "    \n",
    "\n",
    "\n",
    "model_hyperparam_configs = {\n",
    "    \"name\": \"UNetRes3C\",\n",
    "    \"architecture\": \"UNetRes3\",\n",
    "    \"input_config\": \"simple\", \n",
    "    \"batch_size\": 4,\n",
    "    \"lr\": 1e-4,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"use_zeros_weight\": False,\n",
    "    \"zeros_weight\": None,\n",
    "    \"early_stopping\": True,\n",
    "    \"patience\": 10,\n",
    "    \"max_epochs\": 100,\n",
    "    \"notes\": \"Testing classification network\"\n",
    "}\n",
    "\n",
    "in_channels_config = {\n",
    "    \"sea_ice_only\": 14,\n",
    "    \"simple\": 23,\n",
    "    \"all\": 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "split_array, start_prediction_months = generate_split_array()\n",
    "data_directory = os.path.join(DATA_DIRECTORY, 'sicpred/data_pairs_npy')\n",
    "configuration = 'simple'\n",
    "batch_size = 4\n",
    "\n",
    "# Create dataset instances for training, validation, and testing\n",
    "train_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, \\\n",
    "                              mode='classification', split_type='train', target_shape=(336, 320), class_splits=[0.15])\n",
    "val_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months,\\\n",
    "                            mode='classification', split_type='val', target_shape=(336, 320), class_splits=[0.15])\n",
    "test_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, \\\n",
    "                            mode='classification', split_type='test', target_shape=(336, 320), class_splits=[0.15])\n",
    "\n",
    "test_dataset_reg = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, \\\n",
    "                                 split_type='test', target_shape=(336, 320))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 training loss: 478.01440048217773\n",
      "Epoch 1 training loss: 468.74111557006836\n",
      "Epoch 2 training loss: 466.5513150691986\n",
      "Epoch 3 training loss: 465.83489656448364\n",
      "Epoch 4 training loss: 465.3585350513458\n",
      "Epoch 5 training loss: 464.837849855423\n",
      "Epoch 6 training loss: 464.6201808452606\n",
      "Epoch 7 training loss: 464.31439304351807\n",
      "Epoch 8 training loss: 464.2289080619812\n",
      "Epoch 9 training loss: 463.9205758571625\n",
      "Epoch 10 training loss: 463.87258982658386\n",
      "Epoch 11 training loss: 463.6801655292511\n",
      "Epoch 12 training loss: 463.49504804611206\n",
      "Epoch 13 training loss: 463.4763460159302\n",
      "Epoch 14 training loss: 463.16300773620605\n",
      "Epoch 15 training loss: 463.3055942058563\n",
      "Epoch 16 training loss: 463.01252245903015\n",
      "Epoch 17 training loss: 462.9078097343445\n",
      "Epoch 18 training loss: 462.65864515304565\n",
      "Epoch 19 training loss: 462.5199947357178\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNetRes3(in_channels=in_channels_config[model_hyperparam_configs[\"input_config\"]], \\\n",
    "                out_channels=6, mode=\"classification\", device=device, n_channels_factor=1, filter_size=3).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=model_hyperparam_configs[\"lr\"])\n",
    "\n",
    "criterion = CategoricalFocalLoss(device)\n",
    "\n",
    "num_epochs=20 \n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for inputs, targets, target_months in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, target_months)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    print(f'Epoch {epoch} training loss: {running_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, device, model_hyperparam_configs, \n",
    "                plot_training_curve=True, verbose=1, save_val_predictions=True):\n",
    "\n",
    "    # read in some metadata\n",
    "    configuration = model_hyperparam_configs[\"input_config\"]\n",
    "    batch_size = model_hyperparam_configs[\"batch_size\"]\n",
    "    early_stopping = model_hyperparam_configs[\"early_stopping\"]\n",
    "    patience = model_hyperparam_configs[\"patience\"]\n",
    "    model_name = model_hyperparam_configs[\"name\"]\n",
    "    num_epochs = model_hyperparam_configs[\"max_epochs\"]\n",
    "\n",
    "    # generate array labeling which months are train/val/test\n",
    "    split_array, start_prediction_months = generate_split_array()\n",
    "\n",
    "    # create dataset instances for training, validation, and testing\n",
    "    data_pairs_directory = os.path.join(DATA_DIRECTORY, 'sicpred/data_pairs_npy')\n",
    "    train_dataset = SeaIceDataset(data_pairs_directory, configuration, split_array, start_prediction_months, split_type='train', target_shape=(336, 320))\n",
    "    val_dataset = SeaIceDataset(data_pairs_directory, configuration, split_array, start_prediction_months, split_type='val', target_shape=(336, 320))\n",
    "    test_dataset = SeaIceDataset(data_pairs_directory, configuration, split_array, start_prediction_months, split_type='test', target_shape=(336, 320))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_predictions = []\n",
    "    best_val_loss = float('inf')\n",
    "    epoch_at_early_stopping = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, targets, target_months in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets, target_months)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "        if verbose >= 1: print(f\"Epoch [{epoch+1}/{num_epochs}], train loss: {epoch_loss:.4f}\", end=', ')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, target_months in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets, target_months)\n",
    "                val_loss += loss.item() * inputs.size(0)        \n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        if verbose >= 1: print(f\"validation loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if early_stopping:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # Save the best model weights\n",
    "                os.makedirs(f\"{DATA_DIRECTORY}/sicpred/models\", exist_ok=True)\n",
    "                torch.save(model.state_dict(), f\"{DATA_DIRECTORY}/sicpred/models/{model_name}.pth\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    if verbose >= 1: print(\"Early stopping triggered\")\n",
    "                    epoch_at_early_stopping = epoch\n",
    "                    break\n",
    "        \n",
    "    # Plot training curves\n",
    "    if plot_training_curve:\n",
    "        plt.figure(figsize=(6, 3))\n",
    "        plt.plot(np.arange(0.5, len(train_losses) + 0.5, 1), train_losses, label=\"train\")\n",
    "        plt.plot(np.arange(1, len(val_losses) + 1, 1), val_losses, label=\"val\")\n",
    "        plt.legend()\n",
    "        plt.title(model_name)\n",
    "\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.grid()\n",
    "\n",
    "        os.makedirs(f\"{DATA_DIRECTORY}/figures/training_curves\", exist_ok=True)\n",
    "        plt.savefig(f\"{DATA_DIRECTORY}/figures/training_curves/{model_name}_training_curve.png\", bbox_inches='tight', dpi=300)\n",
    "        plt.close()\n",
    "\n",
    "    # Save the validation predictions for hyperparam tuning \n",
    "    if save_val_predictions:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, target_months in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_predictions.append(outputs.cpu().numpy())\n",
    "        os.makedirs(f\"{DATA_DIRECTORY}/sicpred/val_predictions\", exist_ok=True)\n",
    "        np.save(f\"{DATA_DIRECTORY}/sicpred/val_predictions/{model_name}_val_predictions.npy\", np.concatenate(val_predictions, axis=0))\n",
    "\n",
    "    # Save the trained model\n",
    "    if verbose >= 1: print(\"Saving model weights...\", end='')\n",
    "\n",
    "    # record the best val loss and, if early stopping happened, the last epoch\n",
    "    model_hyperparam_configs[\"best_val_loss\"] = best_val_loss\n",
    "\n",
    "    if epoch_at_early_stopping is not None: \n",
    "        model_hyperparam_configs[\"final_epoch\"] = epoch_at_early_stopping\n",
    "    else:\n",
    "        model_hyperparam_configs[\"final_epoch\"] = num_epochs\n",
    "        \n",
    "    os.makedirs(f\"{DATA_DIRECTORY}/sicpred/val_predictions\", exist_ok=True)\n",
    "    save_dict_to_pickle(model_hyperparam_configs, f\"{DATA_DIRECTORY}/sicpred/models/{model_name}.pkl\")\n",
    "    torch.save(model.state_dict(), f\"{DATA_DIRECTORY}/sicpred/models/{model_name}.pth\")\n",
    "    if verbose >= 1: print(\"done! \\n\\n\")\n",
    "\n",
    "    return train_losses, val_losses\n",
    "\n",
    "train_model(model, device, model_hyperparam_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_pickle(load_path):\n",
    "    with open(load_path, 'rb') as file:\n",
    "        data_dict = pickle.load(file)\n",
    "    return data_dict\n",
    "\n",
    "model_name = f\"UNetRes3_simpleinputs_b1lr1\"\n",
    "\n",
    "predictions = np.load(f\"{DATA_DIRECTORY}/sicpred/val_predictions/{model_name}_val_predictions.npy\")\n",
    "\n",
    "with pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nsidc_sic = xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/seaice_conc_monthly_all.nc\")\n",
    "\n",
    "model_state_path = os.path.join(f\"{DATA_DIRECTORY}/sicpred/models/{model_full_name}.pth\")\n",
    "\n",
    "model.load_state_dict(torch.load(model_state_path, map_location=device))\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "inputs_list = []\n",
    "outputs_list = []\n",
    "targets_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, target_months in val_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        inputs_list.append(inputs)\n",
    "        outputs_list.append(outputs)\n",
    "        targets_list.append(targets)\n",
    "        loss = criterion(outputs, targets, target_months)\n",
    "        print(loss.item() * inputs.size(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12.5, 5))\n",
    "\n",
    "month_index = 0\n",
    "lead_time = 0\n",
    "list_elem = 7\n",
    "\n",
    "ax1.contourf(nsidc_sic.xgrid, nsidc_sic.ygrid, outputs_list[list_elem][month_index,lead_time,2:334,2:318].cpu(), cmap='nipy_spectral',\\\n",
    "    levels=np.arange(0,1.025,0.025))\n",
    "cax = ax2.contourf(nsidc_sic.xgrid, nsidc_sic.ygrid, targets_list[list_elem][month_index,lead_time,2:334,2:318].cpu(), cmap='nipy_spectral',\\\n",
    "    levels=np.arange(0,1.025,0.025))\n",
    "\n",
    "ax1.set_title(\"Prediction\")\n",
    "ax2.set_title(\"Truth\")\n",
    "\n",
    "cbar = fig.colorbar(cax, ax=[ax1, ax2], orientation='vertical')\n",
    "cbar.set_label('Sea ice concentration')\n",
    "\n",
    "predicted_month = VAL_MONTHS[month_index + lead_time + list_elem*batch_size]\n",
    "\n",
    "plt.suptitle(rf\"Prediction for {predicted_month.month:02}/{predicted_month.year} at {lead_time+1} lead month\", fontsize=16)\n",
    "#plt.savefig(f\"../figures/sample_predictions/{model_full_name}_{predicted_month.month:02}_{predicted_month.year}_lead_{lead_time+1}.jpg\", dpi=300, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
