{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "# If available, print the name of the GPU\n",
    "if cuda_available:\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "DATA_DIRECTORY = '/oak/stanford/groups/earlew/yuchen'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeaIceDataset(Dataset):\n",
    "    def __init__(self, data_directory, configuration, split_array, start_prediction_months, \\\n",
    "                split_type='train', target_shape=(336, 320)):\n",
    "        self.data_directory = data_directory\n",
    "        self.configuration = configuration\n",
    "        self.split_array = split_array\n",
    "        self.start_prediction_months = start_prediction_months\n",
    "        self.split_type = split_type\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "        # Open the HDF5 files\n",
    "        self.inputs_file = h5py.File(f\"{data_directory}/inputs_{configuration}.h5\", 'r')\n",
    "        self.targets_file = h5py.File(f\"{data_directory}/targets.h5\", 'r')\n",
    "        \n",
    "        self.inputs = self.inputs_file[f\"inputs_{configuration}\"]\n",
    "        self.targets = self.targets_file['targets_sea_ice_only']\n",
    "\n",
    "        self.n_samples, self.n_channels, self.n_y, self.n_x = self.inputs.shape\n",
    "        \n",
    "        # Get indices for the specified split type\n",
    "        self.indices = np.where(self.split_array == split_type)[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        actual_idx = self.indices[idx]\n",
    "        input_data = self.inputs[actual_idx]\n",
    "        target_data = self.targets[actual_idx]\n",
    "        start_prediction_months_subset = self.start_prediction_months[actual_idx]\n",
    "\n",
    "        # Pad input_data and target_data to the target shape\n",
    "        pad_y = self.target_shape[0] - self.n_y\n",
    "        pad_x = self.target_shape[1] - self.n_x\n",
    "        input_data = np.pad(input_data, ((0, 0), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
    "        target_data = np.pad(target_data, ((0, 0), (0, pad_y), (0, pad_x)), mode='constant', constant_values=0)\n",
    "\n",
    "        input_tensor = torch.tensor(input_data, dtype=torch.float32)\n",
    "        target_tensor = torch.tensor(target_data, dtype=torch.float32)\n",
    "\n",
    "        # Get a list of the target months\n",
    "        target_months = []\n",
    "        for i,start_month in enumerate(start_prediction_months_subset):\n",
    "            target_months.append(pd.date_range(start=start_month, end=start_month + pd.DateOffset(months=5), freq=\"MS\"))\n",
    "\n",
    "        \n",
    "        return input_tensor, target_tensor, target_months\n",
    "\n",
    "    def __del__(self):\n",
    "        self.inputs_file.close()\n",
    "        self.targets_file.close()\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, n_filters_factor=1, filter_size=3):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = self.conv_block(in_channels, int(64 * n_filters_factor), filter_size)\n",
    "        self.encoder2 = self.conv_block(int(64 * n_filters_factor), int(128 * n_filters_factor), filter_size)\n",
    "        self.encoder3 = self.conv_block(int(128 * n_filters_factor), int(256 * n_filters_factor), filter_size)\n",
    "        self.bottleneck = self.conv_block(int(256 * n_filters_factor), int(512 * n_filters_factor), filter_size)\n",
    "        \n",
    "        self.decoder1 = self.conv_block(int(512 * n_filters_factor) + int(256 * n_filters_factor), int(256 * n_filters_factor), filter_size)\n",
    "        self.decoder2 = self.conv_block(int(256 * n_filters_factor) + int(128 * n_filters_factor), int(128 * n_filters_factor), filter_size)\n",
    "        self.decoder3 = self.conv_block(int(128 * n_filters_factor) + int(64 * n_filters_factor), int(64 * n_filters_factor), filter_size)\n",
    "        \n",
    "        self.final_conv = nn.Conv2d(int(64 * n_filters_factor), out_channels, kernel_size=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "    def conv_block(self, in_channels, out_channels, filter_size):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=filter_size, padding=filter_size//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool(enc1))\n",
    "        enc3 = self.encoder3(self.pool(enc2))\n",
    "        bottleneck = self.bottleneck(self.pool(enc3))\n",
    "        \n",
    "        dec1 = self.upsample(bottleneck)\n",
    "        dec1 = torch.cat((enc3, dec1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "        \n",
    "        dec2 = self.upsample(dec1)\n",
    "        dec2 = torch.cat((enc2, dec2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        \n",
    "        dec3 = self.upsample(dec2)\n",
    "        dec3 = torch.cat((enc1, dec3), dim=1)\n",
    "        dec3 = self.decoder3(dec3)\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(dec3))\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.ice_mask = xr.open_dataset(f\"{DATA_DIRECTORY}/NSIDC/monthly_ice_mask.nc\").mask\n",
    "\n",
    "    def forward(self, outputs, targets, target_months):\n",
    "        n_active_cells = self.ice_mask.sel(month=target_months.month).sum()\n",
    "\n",
    "        loss = np.sum((targets - outputs) ** 2) / n_active_cells\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 388 (0.78) \n",
      "     val samples: 48 (0.1) \n",
      "     test samples: 61 (0.12)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_MONTHS = pd.date_range(start='1981-01-01', end='2014-12-01', freq='MS')\n",
    "VAL_MONTHS = pd.date_range(start='2015-01-01', end='2018-12-01', freq='MS')\n",
    "TEST_MONTHS = pd.date_range(start='2019-01-01', end='2024-06-01', freq='MS')\n",
    "\n",
    "# Construct the date range for the data pairs \n",
    "# Note that this is not continuous due to the missing data in 1987-1988 \n",
    "first_range = pd.date_range('1981-01', pd.Timestamp('1987-12') - pd.DateOffset(months=6+1), freq='MS')\n",
    "second_range = pd.date_range(pd.Timestamp('1988-01') + pd.DateOffset(months=12+1), '2024-01', freq='MS')\n",
    "start_prediction_months = first_range.append(second_range)\n",
    "\n",
    "split_array = np.empty(np.shape(start_prediction_months), dtype=object)\n",
    "for i,month in enumerate(start_prediction_months):\n",
    "    if month in TRAIN_MONTHS: split_array[i] = \"train\"\n",
    "    if month in VAL_MONTHS: split_array[i] = \"val\"\n",
    "    if month in TEST_MONTHS: split_array[i] = \"test\"\n",
    "\n",
    "def print_split_stats(split_array):\n",
    "    ntrain = sum(split_array == 'train')\n",
    "    nval = sum(split_array == 'val')\n",
    "    ntest = sum(split_array == 'test')\n",
    "    \n",
    "    print(f\"train samples: {ntrain} ({round(ntrain / len(split_array), 2)}) \\n \\\n",
    "    val samples: {nval} ({round(nval / len(split_array), 2)}) \\n \\\n",
    "    test samples: {ntest} ({round(ntest / len(split_array), 2)})\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(in_channels=23, out_channels=6, n_filters_factor=1, filter_size=3).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "print_split_stats(split_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = os.path.join(DATA_DIRECTORY, 'sicpred/data_pairs_npy')\n",
    "configuration = 'simple'\n",
    "batch_size = 32 \n",
    "\n",
    "# Create dataset instances for training, validation, and testing\n",
    "train_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, split_type='train', target_shape=(336, 320))\n",
    "val_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, split_type='val', target_shape=(336, 320))\n",
    "test_dataset = SeaIceDataset(data_directory, configuration, split_array, start_prediction_months, split_type='test', target_shape=(336, 320))\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8223\n",
      "Validation Loss: 1.0283\n",
      "Epoch [2/10], Loss: 0.6359\n",
      "Validation Loss: 1.0328\n",
      "Epoch [3/10], Loss: 0.5942\n",
      "Validation Loss: 0.6964\n",
      "Epoch [4/10], Loss: 0.5759\n",
      "Validation Loss: 0.5820\n",
      "Epoch [5/10], Loss: 0.5660\n",
      "Validation Loss: 0.5673\n",
      "Epoch [6/10], Loss: 0.5585\n",
      "Validation Loss: 0.5599\n",
      "Epoch [7/10], Loss: 0.5529\n",
      "Validation Loss: 0.5574\n",
      "Epoch [8/10], Loss: 0.5479\n",
      "Validation Loss: 0.5524\n",
      "Epoch [9/10], Loss: 0.5425\n",
      "Validation Loss: 0.5464\n",
      "Epoch [10/10], Loss: 0.5379\n",
      "Validation Loss: 0.5432\n",
      "Test Loss: 0.5488\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, targets, target_months in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Testing loop (optional)\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "os.makedirs(f\"{DATA_DIRECTORY}/sicpred/models\", exist_ok=True)\n",
    "torch.save(model.state_dict(), f'{DATA_DIRECTORY}/sicpred/models/unet_resolution3_{configuration}_lr1e-4_epoch{num_epochs}_adam.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_simple = np.load(f\"{DATA_DIRECTORY}/sicpred/data_pairs_npy/inputs_sea_ice_only.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 332, 316, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(inputs_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet(input_shape, loss, learning_rate=1e-4, n_filters_factor=1, filter_size=3):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    conv1 = Conv2D(int(64 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    conv1 = Conv2D(int(64 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv1)\n",
    "    bn1 = BatchNormalization(axis=-1)(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(bn1)\n",
    "\n",
    "    conv2 = Conv2D(int(128 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(pool1)\n",
    "    conv2 = Conv2D(int(128 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv2)\n",
    "    bn2 = BatchNormalization(axis=-1)(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(bn2)\n",
    "\n",
    "    conv3 = Conv2D(int(256 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(pool2)\n",
    "    conv3 = Conv2D(int(256 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv3)\n",
    "    bn3 = BatchNormalization(axis=-1)(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(bn3)\n",
    "\n",
    "    conv4 = Conv2D(int(512 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(pool3)\n",
    "    conv4 = Conv2D(int(512 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv4)\n",
    "    bn4 = BatchNormalization(axis=-1)(conv4)\n",
    "\n",
    "    up5 = Conv2D(int(256 * n_filters_factor), 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(bn4))\n",
    "    merge5 = concatenate([bn3, up5], axis=3)\n",
    "    conv5 = Conv2D(int(256 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(merge5)\n",
    "    conv5 = Conv2D(int(256 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv5)\n",
    "    bn5 = BatchNormalization(axis=-1)(conv5)\n",
    "\n",
    "    up6 = Conv2D(int(128 * n_filters_factor), 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(bn5))\n",
    "    merge6 = concatenate([bn2,up6], axis=3)\n",
    "    conv6 = Conv2D(int(128 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(merge6)\n",
    "    conv6 = Conv2D(int(128 * n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv6)\n",
    "    bn6 = BatchNormalization(axis=-1)(conv6)\n",
    "\n",
    "    up7 = Conv2D(int(64*n_filters_factor), 2, activation='relu', padding='same', kernel_initializer='he_normal')(UpSampling2D(size=(2,2), interpolation='nearest')(bn6))\n",
    "    merge7 = concatenate([bn1,up7], axis=3)\n",
    "    conv7 = Conv2D(int(64*n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(merge7)\n",
    "    conv7 = Conv2D(int(64*n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "    conv7 = Conv2D(int(64*n_filters_factor), filter_size, activation='relu', padding='same', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    final_layer = Conv2D(1, (1, 1), activation='sigmoid', kernel_initializer='he_normal')(conv7)\n",
    "\n",
    "    model = Model(inputs, final_layer)\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss=loss)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Functional name=functional_3, built=True>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet(np.shape(inputs_cropped)[1:], MeanSquaredError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
